{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,Activation,Dropout,Conv2D,MaxPool2D,Flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import set_random_seed\n",
    "set_random_seed(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.random import seed\n",
    "seed(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_width,input_height=224,224"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape=(input_width,input_height,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir='fruits/train'\n",
    "test_dir='fruits/test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=Sequential()\n",
    "model.add(Conv2D(32,(3,3),input_shape=input_shape))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPool2D(pool_size=(2,2)))\n",
    "model.add(Conv2D(32,(3,3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPool2D(pool_size=(2,2)))\n",
    "model.add(Conv2D(64,(3,3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPool2D(pool_size=(2,2)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(32))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(5,activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on module keras.optimizers in keras:\n",
      "\n",
      "NAME\n",
      "    keras.optimizers - Built-in optimizer classes.\n",
      "\n",
      "CLASSES\n",
      "    builtins.object\n",
      "        Optimizer\n",
      "            Adadelta\n",
      "            Adagrad\n",
      "            Adam\n",
      "            Adamax\n",
      "            Nadam\n",
      "            RMSprop\n",
      "            SGD\n",
      "            TFOptimizer\n",
      "    \n",
      "    class Adadelta(Optimizer)\n",
      "     |  Adadelta optimizer.\n",
      "     |  \n",
      "     |  Adadelta is a more robust extension of Adagrad\n",
      "     |  that adapts learning rates based on a moving window of gradient updates,\n",
      "     |  instead of accumulating all past gradients. This way, Adadelta continues\n",
      "     |  learning even when many updates have been done. Compared to Adagrad, in the\n",
      "     |  original version of Adadelta you don't have to set an initial learning\n",
      "     |  rate. In this version, initial learning rate and decay factor can\n",
      "     |  be set, as in most other Keras optimizers.\n",
      "     |  \n",
      "     |  It is recommended to leave the parameters of this optimizer\n",
      "     |  at their default values.\n",
      "     |  \n",
      "     |  # Arguments\n",
      "     |      lr: float >= 0. Initial learning rate, defaults to 1.\n",
      "     |          It is recommended to leave it at the default value.\n",
      "     |      rho: float >= 0. Adadelta decay factor, corresponding to fraction of\n",
      "     |          gradient to keep at each time step.\n",
      "     |      epsilon: float >= 0. Fuzz factor. If `None`, defaults to `K.epsilon()`.\n",
      "     |      decay: float >= 0. Initial learning rate decay.\n",
      "     |  \n",
      "     |  # References\n",
      "     |      - [Adadelta - an adaptive learning rate method]\n",
      "     |        (https://arxiv.org/abs/1212.5701)\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      Adadelta\n",
      "     |      Optimizer\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, lr=1.0, rho=0.95, epsilon=None, decay=0.0, **kwargs)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  get_config(self)\n",
      "     |  \n",
      "     |  get_updates(self, loss, params)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from Optimizer:\n",
      "     |  \n",
      "     |  get_gradients(self, loss, params)\n",
      "     |  \n",
      "     |  get_weights(self)\n",
      "     |      Returns the current value of the weights of the optimizer.\n",
      "     |      \n",
      "     |      # Returns\n",
      "     |          A list of numpy arrays.\n",
      "     |  \n",
      "     |  set_weights(self, weights)\n",
      "     |      Sets the weights of the optimizer, from Numpy arrays.\n",
      "     |      \n",
      "     |      Should only be called after computing the gradients\n",
      "     |      (otherwise the optimizer has no weights).\n",
      "     |      \n",
      "     |      # Arguments\n",
      "     |          weights: a list of Numpy arrays. The number\n",
      "     |              of arrays and their shape must match\n",
      "     |              number of the dimensions of the weights\n",
      "     |              of the optimizer (i.e. it should match the\n",
      "     |              output of `get_weights`).\n",
      "     |      \n",
      "     |      # Raises\n",
      "     |          ValueError: in case of incompatible weight shapes.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from Optimizer:\n",
      "     |  \n",
      "     |  from_config(config) from builtins.type\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from Optimizer:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class Adagrad(Optimizer)\n",
      "     |  Adagrad optimizer.\n",
      "     |  \n",
      "     |  Adagrad is an optimizer with parameter-specific learning rates,\n",
      "     |  which are adapted relative to how frequently a parameter gets\n",
      "     |  updated during training. The more updates a parameter receives,\n",
      "     |  the smaller the updates.\n",
      "     |  \n",
      "     |  It is recommended to leave the parameters of this optimizer\n",
      "     |  at their default values.\n",
      "     |  \n",
      "     |  # Arguments\n",
      "     |      lr: float >= 0. Initial learning rate.\n",
      "     |      epsilon: float >= 0. If `None`, defaults to `K.epsilon()`.\n",
      "     |      decay: float >= 0. Learning rate decay over each update.\n",
      "     |  \n",
      "     |  # References\n",
      "     |      - [Adaptive Subgradient Methods for Online Learning and Stochastic\n",
      "     |         Optimization](http://www.jmlr.org/papers/volume12/duchi11a/duchi11a.pdf)\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      Adagrad\n",
      "     |      Optimizer\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, lr=0.01, epsilon=None, decay=0.0, **kwargs)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  get_config(self)\n",
      "     |  \n",
      "     |  get_updates(self, loss, params)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from Optimizer:\n",
      "     |  \n",
      "     |  get_gradients(self, loss, params)\n",
      "     |  \n",
      "     |  get_weights(self)\n",
      "     |      Returns the current value of the weights of the optimizer.\n",
      "     |      \n",
      "     |      # Returns\n",
      "     |          A list of numpy arrays.\n",
      "     |  \n",
      "     |  set_weights(self, weights)\n",
      "     |      Sets the weights of the optimizer, from Numpy arrays.\n",
      "     |      \n",
      "     |      Should only be called after computing the gradients\n",
      "     |      (otherwise the optimizer has no weights).\n",
      "     |      \n",
      "     |      # Arguments\n",
      "     |          weights: a list of Numpy arrays. The number\n",
      "     |              of arrays and their shape must match\n",
      "     |              number of the dimensions of the weights\n",
      "     |              of the optimizer (i.e. it should match the\n",
      "     |              output of `get_weights`).\n",
      "     |      \n",
      "     |      # Raises\n",
      "     |          ValueError: in case of incompatible weight shapes.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from Optimizer:\n",
      "     |  \n",
      "     |  from_config(config) from builtins.type\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from Optimizer:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class Adam(Optimizer)\n",
      "     |  Adam optimizer.\n",
      "     |  \n",
      "     |  Default parameters follow those provided in the original paper.\n",
      "     |  \n",
      "     |  # Arguments\n",
      "     |      lr: float >= 0. Learning rate.\n",
      "     |      beta_1: float, 0 < beta < 1. Generally close to 1.\n",
      "     |      beta_2: float, 0 < beta < 1. Generally close to 1.\n",
      "     |      epsilon: float >= 0. Fuzz factor. If `None`, defaults to `K.epsilon()`.\n",
      "     |      decay: float >= 0. Learning rate decay over each update.\n",
      "     |      amsgrad: boolean. Whether to apply the AMSGrad variant of this\n",
      "     |          algorithm from the paper \"On the Convergence of Adam and\n",
      "     |          Beyond\".\n",
      "     |  \n",
      "     |  # References\n",
      "     |      - [Adam - A Method for Stochastic Optimization]\n",
      "     |        (https://arxiv.org/abs/1412.6980v8)\n",
      "     |      - [On the Convergence of Adam and Beyond]\n",
      "     |        (https://openreview.net/forum?id=ryQu7f-RZ)\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      Adam\n",
      "     |      Optimizer\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False, **kwargs)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  get_config(self)\n",
      "     |  \n",
      "     |  get_updates(self, loss, params)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from Optimizer:\n",
      "     |  \n",
      "     |  get_gradients(self, loss, params)\n",
      "     |  \n",
      "     |  get_weights(self)\n",
      "     |      Returns the current value of the weights of the optimizer.\n",
      "     |      \n",
      "     |      # Returns\n",
      "     |          A list of numpy arrays.\n",
      "     |  \n",
      "     |  set_weights(self, weights)\n",
      "     |      Sets the weights of the optimizer, from Numpy arrays.\n",
      "     |      \n",
      "     |      Should only be called after computing the gradients\n",
      "     |      (otherwise the optimizer has no weights).\n",
      "     |      \n",
      "     |      # Arguments\n",
      "     |          weights: a list of Numpy arrays. The number\n",
      "     |              of arrays and their shape must match\n",
      "     |              number of the dimensions of the weights\n",
      "     |              of the optimizer (i.e. it should match the\n",
      "     |              output of `get_weights`).\n",
      "     |      \n",
      "     |      # Raises\n",
      "     |          ValueError: in case of incompatible weight shapes.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from Optimizer:\n",
      "     |  \n",
      "     |  from_config(config) from builtins.type\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from Optimizer:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class Adamax(Optimizer)\n",
      "     |  Adamax optimizer from Adam paper's Section 7.\n",
      "     |  \n",
      "     |  It is a variant of Adam based on the infinity norm.\n",
      "     |  Default parameters follow those provided in the paper.\n",
      "     |  \n",
      "     |  # Arguments\n",
      "     |      lr: float >= 0. Learning rate.\n",
      "     |      beta_1/beta_2: floats, 0 < beta < 1. Generally close to 1.\n",
      "     |      epsilon: float >= 0. Fuzz factor. If `None`, defaults to `K.epsilon()`.\n",
      "     |      decay: float >= 0. Learning rate decay over each update.\n",
      "     |  \n",
      "     |  # References\n",
      "     |      - [Adam - A Method for Stochastic Optimization]\n",
      "     |        (https://arxiv.org/abs/1412.6980v8)\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      Adamax\n",
      "     |      Optimizer\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, lr=0.002, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, **kwargs)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  get_config(self)\n",
      "     |  \n",
      "     |  get_updates(self, loss, params)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from Optimizer:\n",
      "     |  \n",
      "     |  get_gradients(self, loss, params)\n",
      "     |  \n",
      "     |  get_weights(self)\n",
      "     |      Returns the current value of the weights of the optimizer.\n",
      "     |      \n",
      "     |      # Returns\n",
      "     |          A list of numpy arrays.\n",
      "     |  \n",
      "     |  set_weights(self, weights)\n",
      "     |      Sets the weights of the optimizer, from Numpy arrays.\n",
      "     |      \n",
      "     |      Should only be called after computing the gradients\n",
      "     |      (otherwise the optimizer has no weights).\n",
      "     |      \n",
      "     |      # Arguments\n",
      "     |          weights: a list of Numpy arrays. The number\n",
      "     |              of arrays and their shape must match\n",
      "     |              number of the dimensions of the weights\n",
      "     |              of the optimizer (i.e. it should match the\n",
      "     |              output of `get_weights`).\n",
      "     |      \n",
      "     |      # Raises\n",
      "     |          ValueError: in case of incompatible weight shapes.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from Optimizer:\n",
      "     |  \n",
      "     |  from_config(config) from builtins.type\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from Optimizer:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class Nadam(Optimizer)\n",
      "     |  Nesterov Adam optimizer.\n",
      "     |  \n",
      "     |  Much like Adam is essentially RMSprop with momentum,\n",
      "     |  Nadam is Adam RMSprop with Nesterov momentum.\n",
      "     |  \n",
      "     |  Default parameters follow those provided in the paper.\n",
      "     |  It is recommended to leave the parameters of this optimizer\n",
      "     |  at their default values.\n",
      "     |  \n",
      "     |  # Arguments\n",
      "     |      lr: float >= 0. Learning rate.\n",
      "     |      beta_1/beta_2: floats, 0 < beta < 1. Generally close to 1.\n",
      "     |      epsilon: float >= 0. Fuzz factor. If `None`, defaults to `K.epsilon()`.\n",
      "     |  \n",
      "     |  # References\n",
      "     |      - [Nadam report](http://cs229.stanford.edu/proj2015/054_report.pdf)\n",
      "     |      - [On the importance of initialization and momentum in deep learning]\n",
      "     |        (http://www.cs.toronto.edu/~fritz/absps/momentum.pdf)\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      Nadam\n",
      "     |      Optimizer\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, lr=0.002, beta_1=0.9, beta_2=0.999, epsilon=None, schedule_decay=0.004, **kwargs)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  get_config(self)\n",
      "     |  \n",
      "     |  get_updates(self, loss, params)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from Optimizer:\n",
      "     |  \n",
      "     |  get_gradients(self, loss, params)\n",
      "     |  \n",
      "     |  get_weights(self)\n",
      "     |      Returns the current value of the weights of the optimizer.\n",
      "     |      \n",
      "     |      # Returns\n",
      "     |          A list of numpy arrays.\n",
      "     |  \n",
      "     |  set_weights(self, weights)\n",
      "     |      Sets the weights of the optimizer, from Numpy arrays.\n",
      "     |      \n",
      "     |      Should only be called after computing the gradients\n",
      "     |      (otherwise the optimizer has no weights).\n",
      "     |      \n",
      "     |      # Arguments\n",
      "     |          weights: a list of Numpy arrays. The number\n",
      "     |              of arrays and their shape must match\n",
      "     |              number of the dimensions of the weights\n",
      "     |              of the optimizer (i.e. it should match the\n",
      "     |              output of `get_weights`).\n",
      "     |      \n",
      "     |      # Raises\n",
      "     |          ValueError: in case of incompatible weight shapes.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from Optimizer:\n",
      "     |  \n",
      "     |  from_config(config) from builtins.type\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from Optimizer:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class Optimizer(builtins.object)\n",
      "     |  Abstract optimizer base class.\n",
      "     |  \n",
      "     |  Note: this is the parent class of all optimizers, not an actual optimizer\n",
      "     |  that can be used for training models.\n",
      "     |  \n",
      "     |  All Keras optimizers support the following keyword arguments:\n",
      "     |  \n",
      "     |      clipnorm: float >= 0. Gradients will be clipped\n",
      "     |          when their L2 norm exceeds this value.\n",
      "     |      clipvalue: float >= 0. Gradients will be clipped\n",
      "     |          when their absolute value exceeds this value.\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, **kwargs)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  get_config(self)\n",
      "     |  \n",
      "     |  get_gradients(self, loss, params)\n",
      "     |  \n",
      "     |  get_updates(self, loss, params)\n",
      "     |  \n",
      "     |  get_weights(self)\n",
      "     |      Returns the current value of the weights of the optimizer.\n",
      "     |      \n",
      "     |      # Returns\n",
      "     |          A list of numpy arrays.\n",
      "     |  \n",
      "     |  set_weights(self, weights)\n",
      "     |      Sets the weights of the optimizer, from Numpy arrays.\n",
      "     |      \n",
      "     |      Should only be called after computing the gradients\n",
      "     |      (otherwise the optimizer has no weights).\n",
      "     |      \n",
      "     |      # Arguments\n",
      "     |          weights: a list of Numpy arrays. The number\n",
      "     |              of arrays and their shape must match\n",
      "     |              number of the dimensions of the weights\n",
      "     |              of the optimizer (i.e. it should match the\n",
      "     |              output of `get_weights`).\n",
      "     |      \n",
      "     |      # Raises\n",
      "     |          ValueError: in case of incompatible weight shapes.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods defined here:\n",
      "     |  \n",
      "     |  from_config(config) from builtins.type\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class RMSprop(Optimizer)\n",
      "     |  RMSProp optimizer.\n",
      "     |  \n",
      "     |  It is recommended to leave the parameters of this optimizer\n",
      "     |  at their default values\n",
      "     |  (except the learning rate, which can be freely tuned).\n",
      "     |  \n",
      "     |  This optimizer is usually a good choice for recurrent\n",
      "     |  neural networks.\n",
      "     |  \n",
      "     |  # Arguments\n",
      "     |      lr: float >= 0. Learning rate.\n",
      "     |      rho: float >= 0.\n",
      "     |      epsilon: float >= 0. Fuzz factor. If `None`, defaults to `K.epsilon()`.\n",
      "     |      decay: float >= 0. Learning rate decay over each update.\n",
      "     |  \n",
      "     |  # References\n",
      "     |      - [rmsprop: Divide the gradient by a running average of its recent magnitude]\n",
      "     |        (http://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf)\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      RMSprop\n",
      "     |      Optimizer\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, lr=0.001, rho=0.9, epsilon=None, decay=0.0, **kwargs)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  get_config(self)\n",
      "     |  \n",
      "     |  get_updates(self, loss, params)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from Optimizer:\n",
      "     |  \n",
      "     |  get_gradients(self, loss, params)\n",
      "     |  \n",
      "     |  get_weights(self)\n",
      "     |      Returns the current value of the weights of the optimizer.\n",
      "     |      \n",
      "     |      # Returns\n",
      "     |          A list of numpy arrays.\n",
      "     |  \n",
      "     |  set_weights(self, weights)\n",
      "     |      Sets the weights of the optimizer, from Numpy arrays.\n",
      "     |      \n",
      "     |      Should only be called after computing the gradients\n",
      "     |      (otherwise the optimizer has no weights).\n",
      "     |      \n",
      "     |      # Arguments\n",
      "     |          weights: a list of Numpy arrays. The number\n",
      "     |              of arrays and their shape must match\n",
      "     |              number of the dimensions of the weights\n",
      "     |              of the optimizer (i.e. it should match the\n",
      "     |              output of `get_weights`).\n",
      "     |      \n",
      "     |      # Raises\n",
      "     |          ValueError: in case of incompatible weight shapes.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from Optimizer:\n",
      "     |  \n",
      "     |  from_config(config) from builtins.type\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from Optimizer:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class SGD(Optimizer)\n",
      "     |  Stochastic gradient descent optimizer.\n",
      "     |  \n",
      "     |  Includes support for momentum,\n",
      "     |  learning rate decay, and Nesterov momentum.\n",
      "     |  \n",
      "     |  # Arguments\n",
      "     |      lr: float >= 0. Learning rate.\n",
      "     |      momentum: float >= 0. Parameter that accelerates SGD\n",
      "     |          in the relevant direction and dampens oscillations.\n",
      "     |      decay: float >= 0. Learning rate decay over each update.\n",
      "     |      nesterov: boolean. Whether to apply Nesterov momentum.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      SGD\n",
      "     |      Optimizer\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, lr=0.01, momentum=0.0, decay=0.0, nesterov=False, **kwargs)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  get_config(self)\n",
      "     |  \n",
      "     |  get_updates(self, loss, params)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from Optimizer:\n",
      "     |  \n",
      "     |  get_gradients(self, loss, params)\n",
      "     |  \n",
      "     |  get_weights(self)\n",
      "     |      Returns the current value of the weights of the optimizer.\n",
      "     |      \n",
      "     |      # Returns\n",
      "     |          A list of numpy arrays.\n",
      "     |  \n",
      "     |  set_weights(self, weights)\n",
      "     |      Sets the weights of the optimizer, from Numpy arrays.\n",
      "     |      \n",
      "     |      Should only be called after computing the gradients\n",
      "     |      (otherwise the optimizer has no weights).\n",
      "     |      \n",
      "     |      # Arguments\n",
      "     |          weights: a list of Numpy arrays. The number\n",
      "     |              of arrays and their shape must match\n",
      "     |              number of the dimensions of the weights\n",
      "     |              of the optimizer (i.e. it should match the\n",
      "     |              output of `get_weights`).\n",
      "     |      \n",
      "     |      # Raises\n",
      "     |          ValueError: in case of incompatible weight shapes.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from Optimizer:\n",
      "     |  \n",
      "     |  from_config(config) from builtins.type\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from Optimizer:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class TFOptimizer(Optimizer)\n",
      "     |  Wrapper class for native TensorFlow optimizers.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      TFOptimizer\n",
      "     |      Optimizer\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, optimizer)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  from_config(self, config)\n",
      "     |  \n",
      "     |  get_config(self)\n",
      "     |  \n",
      "     |  get_updates(self, loss, params)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  weights\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from Optimizer:\n",
      "     |  \n",
      "     |  get_gradients(self, loss, params)\n",
      "     |  \n",
      "     |  get_weights(self)\n",
      "     |      Returns the current value of the weights of the optimizer.\n",
      "     |      \n",
      "     |      # Returns\n",
      "     |          A list of numpy arrays.\n",
      "     |  \n",
      "     |  set_weights(self, weights)\n",
      "     |      Sets the weights of the optimizer, from Numpy arrays.\n",
      "     |      \n",
      "     |      Should only be called after computing the gradients\n",
      "     |      (otherwise the optimizer has no weights).\n",
      "     |      \n",
      "     |      # Arguments\n",
      "     |          weights: a list of Numpy arrays. The number\n",
      "     |              of arrays and their shape must match\n",
      "     |              number of the dimensions of the weights\n",
      "     |              of the optimizer (i.e. it should match the\n",
      "     |              output of `get_weights`).\n",
      "     |      \n",
      "     |      # Raises\n",
      "     |          ValueError: in case of incompatible weight shapes.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from Optimizer:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    adadelta = class Adadelta(Optimizer)\n",
      "     |  Adadelta optimizer.\n",
      "     |  \n",
      "     |  Adadelta is a more robust extension of Adagrad\n",
      "     |  that adapts learning rates based on a moving window of gradient updates,\n",
      "     |  instead of accumulating all past gradients. This way, Adadelta continues\n",
      "     |  learning even when many updates have been done. Compared to Adagrad, in the\n",
      "     |  original version of Adadelta you don't have to set an initial learning\n",
      "     |  rate. In this version, initial learning rate and decay factor can\n",
      "     |  be set, as in most other Keras optimizers.\n",
      "     |  \n",
      "     |  It is recommended to leave the parameters of this optimizer\n",
      "     |  at their default values.\n",
      "     |  \n",
      "     |  # Arguments\n",
      "     |      lr: float >= 0. Initial learning rate, defaults to 1.\n",
      "     |          It is recommended to leave it at the default value.\n",
      "     |      rho: float >= 0. Adadelta decay factor, corresponding to fraction of\n",
      "     |          gradient to keep at each time step.\n",
      "     |      epsilon: float >= 0. Fuzz factor. If `None`, defaults to `K.epsilon()`.\n",
      "     |      decay: float >= 0. Initial learning rate decay.\n",
      "     |  \n",
      "     |  # References\n",
      "     |      - [Adadelta - an adaptive learning rate method]\n",
      "     |        (https://arxiv.org/abs/1212.5701)\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      Adadelta\n",
      "     |      Optimizer\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, lr=1.0, rho=0.95, epsilon=None, decay=0.0, **kwargs)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  get_config(self)\n",
      "     |  \n",
      "     |  get_updates(self, loss, params)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from Optimizer:\n",
      "     |  \n",
      "     |  get_gradients(self, loss, params)\n",
      "     |  \n",
      "     |  get_weights(self)\n",
      "     |      Returns the current value of the weights of the optimizer.\n",
      "     |      \n",
      "     |      # Returns\n",
      "     |          A list of numpy arrays.\n",
      "     |  \n",
      "     |  set_weights(self, weights)\n",
      "     |      Sets the weights of the optimizer, from Numpy arrays.\n",
      "     |      \n",
      "     |      Should only be called after computing the gradients\n",
      "     |      (otherwise the optimizer has no weights).\n",
      "     |      \n",
      "     |      # Arguments\n",
      "     |          weights: a list of Numpy arrays. The number\n",
      "     |              of arrays and their shape must match\n",
      "     |              number of the dimensions of the weights\n",
      "     |              of the optimizer (i.e. it should match the\n",
      "     |              output of `get_weights`).\n",
      "     |      \n",
      "     |      # Raises\n",
      "     |          ValueError: in case of incompatible weight shapes.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from Optimizer:\n",
      "     |  \n",
      "     |  from_config(config) from builtins.type\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from Optimizer:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    adagrad = class Adagrad(Optimizer)\n",
      "     |  Adagrad optimizer.\n",
      "     |  \n",
      "     |  Adagrad is an optimizer with parameter-specific learning rates,\n",
      "     |  which are adapted relative to how frequently a parameter gets\n",
      "     |  updated during training. The more updates a parameter receives,\n",
      "     |  the smaller the updates.\n",
      "     |  \n",
      "     |  It is recommended to leave the parameters of this optimizer\n",
      "     |  at their default values.\n",
      "     |  \n",
      "     |  # Arguments\n",
      "     |      lr: float >= 0. Initial learning rate.\n",
      "     |      epsilon: float >= 0. If `None`, defaults to `K.epsilon()`.\n",
      "     |      decay: float >= 0. Learning rate decay over each update.\n",
      "     |  \n",
      "     |  # References\n",
      "     |      - [Adaptive Subgradient Methods for Online Learning and Stochastic\n",
      "     |         Optimization](http://www.jmlr.org/papers/volume12/duchi11a/duchi11a.pdf)\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      Adagrad\n",
      "     |      Optimizer\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, lr=0.01, epsilon=None, decay=0.0, **kwargs)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  get_config(self)\n",
      "     |  \n",
      "     |  get_updates(self, loss, params)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from Optimizer:\n",
      "     |  \n",
      "     |  get_gradients(self, loss, params)\n",
      "     |  \n",
      "     |  get_weights(self)\n",
      "     |      Returns the current value of the weights of the optimizer.\n",
      "     |      \n",
      "     |      # Returns\n",
      "     |          A list of numpy arrays.\n",
      "     |  \n",
      "     |  set_weights(self, weights)\n",
      "     |      Sets the weights of the optimizer, from Numpy arrays.\n",
      "     |      \n",
      "     |      Should only be called after computing the gradients\n",
      "     |      (otherwise the optimizer has no weights).\n",
      "     |      \n",
      "     |      # Arguments\n",
      "     |          weights: a list of Numpy arrays. The number\n",
      "     |              of arrays and their shape must match\n",
      "     |              number of the dimensions of the weights\n",
      "     |              of the optimizer (i.e. it should match the\n",
      "     |              output of `get_weights`).\n",
      "     |      \n",
      "     |      # Raises\n",
      "     |          ValueError: in case of incompatible weight shapes.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from Optimizer:\n",
      "     |  \n",
      "     |  from_config(config) from builtins.type\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from Optimizer:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    adam = class Adam(Optimizer)\n",
      "     |  Adam optimizer.\n",
      "     |  \n",
      "     |  Default parameters follow those provided in the original paper.\n",
      "     |  \n",
      "     |  # Arguments\n",
      "     |      lr: float >= 0. Learning rate.\n",
      "     |      beta_1: float, 0 < beta < 1. Generally close to 1.\n",
      "     |      beta_2: float, 0 < beta < 1. Generally close to 1.\n",
      "     |      epsilon: float >= 0. Fuzz factor. If `None`, defaults to `K.epsilon()`.\n",
      "     |      decay: float >= 0. Learning rate decay over each update.\n",
      "     |      amsgrad: boolean. Whether to apply the AMSGrad variant of this\n",
      "     |          algorithm from the paper \"On the Convergence of Adam and\n",
      "     |          Beyond\".\n",
      "     |  \n",
      "     |  # References\n",
      "     |      - [Adam - A Method for Stochastic Optimization]\n",
      "     |        (https://arxiv.org/abs/1412.6980v8)\n",
      "     |      - [On the Convergence of Adam and Beyond]\n",
      "     |        (https://openreview.net/forum?id=ryQu7f-RZ)\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      Adam\n",
      "     |      Optimizer\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False, **kwargs)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  get_config(self)\n",
      "     |  \n",
      "     |  get_updates(self, loss, params)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from Optimizer:\n",
      "     |  \n",
      "     |  get_gradients(self, loss, params)\n",
      "     |  \n",
      "     |  get_weights(self)\n",
      "     |      Returns the current value of the weights of the optimizer.\n",
      "     |      \n",
      "     |      # Returns\n",
      "     |          A list of numpy arrays.\n",
      "     |  \n",
      "     |  set_weights(self, weights)\n",
      "     |      Sets the weights of the optimizer, from Numpy arrays.\n",
      "     |      \n",
      "     |      Should only be called after computing the gradients\n",
      "     |      (otherwise the optimizer has no weights).\n",
      "     |      \n",
      "     |      # Arguments\n",
      "     |          weights: a list of Numpy arrays. The number\n",
      "     |              of arrays and their shape must match\n",
      "     |              number of the dimensions of the weights\n",
      "     |              of the optimizer (i.e. it should match the\n",
      "     |              output of `get_weights`).\n",
      "     |      \n",
      "     |      # Raises\n",
      "     |          ValueError: in case of incompatible weight shapes.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from Optimizer:\n",
      "     |  \n",
      "     |  from_config(config) from builtins.type\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from Optimizer:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    adamax = class Adamax(Optimizer)\n",
      "     |  Adamax optimizer from Adam paper's Section 7.\n",
      "     |  \n",
      "     |  It is a variant of Adam based on the infinity norm.\n",
      "     |  Default parameters follow those provided in the paper.\n",
      "     |  \n",
      "     |  # Arguments\n",
      "     |      lr: float >= 0. Learning rate.\n",
      "     |      beta_1/beta_2: floats, 0 < beta < 1. Generally close to 1.\n",
      "     |      epsilon: float >= 0. Fuzz factor. If `None`, defaults to `K.epsilon()`.\n",
      "     |      decay: float >= 0. Learning rate decay over each update.\n",
      "     |  \n",
      "     |  # References\n",
      "     |      - [Adam - A Method for Stochastic Optimization]\n",
      "     |        (https://arxiv.org/abs/1412.6980v8)\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      Adamax\n",
      "     |      Optimizer\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, lr=0.002, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, **kwargs)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  get_config(self)\n",
      "     |  \n",
      "     |  get_updates(self, loss, params)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from Optimizer:\n",
      "     |  \n",
      "     |  get_gradients(self, loss, params)\n",
      "     |  \n",
      "     |  get_weights(self)\n",
      "     |      Returns the current value of the weights of the optimizer.\n",
      "     |      \n",
      "     |      # Returns\n",
      "     |          A list of numpy arrays.\n",
      "     |  \n",
      "     |  set_weights(self, weights)\n",
      "     |      Sets the weights of the optimizer, from Numpy arrays.\n",
      "     |      \n",
      "     |      Should only be called after computing the gradients\n",
      "     |      (otherwise the optimizer has no weights).\n",
      "     |      \n",
      "     |      # Arguments\n",
      "     |          weights: a list of Numpy arrays. The number\n",
      "     |              of arrays and their shape must match\n",
      "     |              number of the dimensions of the weights\n",
      "     |              of the optimizer (i.e. it should match the\n",
      "     |              output of `get_weights`).\n",
      "     |      \n",
      "     |      # Raises\n",
      "     |          ValueError: in case of incompatible weight shapes.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from Optimizer:\n",
      "     |  \n",
      "     |  from_config(config) from builtins.type\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from Optimizer:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    nadam = class Nadam(Optimizer)\n",
      "     |  Nesterov Adam optimizer.\n",
      "     |  \n",
      "     |  Much like Adam is essentially RMSprop with momentum,\n",
      "     |  Nadam is Adam RMSprop with Nesterov momentum.\n",
      "     |  \n",
      "     |  Default parameters follow those provided in the paper.\n",
      "     |  It is recommended to leave the parameters of this optimizer\n",
      "     |  at their default values.\n",
      "     |  \n",
      "     |  # Arguments\n",
      "     |      lr: float >= 0. Learning rate.\n",
      "     |      beta_1/beta_2: floats, 0 < beta < 1. Generally close to 1.\n",
      "     |      epsilon: float >= 0. Fuzz factor. If `None`, defaults to `K.epsilon()`.\n",
      "     |  \n",
      "     |  # References\n",
      "     |      - [Nadam report](http://cs229.stanford.edu/proj2015/054_report.pdf)\n",
      "     |      - [On the importance of initialization and momentum in deep learning]\n",
      "     |        (http://www.cs.toronto.edu/~fritz/absps/momentum.pdf)\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      Nadam\n",
      "     |      Optimizer\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, lr=0.002, beta_1=0.9, beta_2=0.999, epsilon=None, schedule_decay=0.004, **kwargs)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  get_config(self)\n",
      "     |  \n",
      "     |  get_updates(self, loss, params)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from Optimizer:\n",
      "     |  \n",
      "     |  get_gradients(self, loss, params)\n",
      "     |  \n",
      "     |  get_weights(self)\n",
      "     |      Returns the current value of the weights of the optimizer.\n",
      "     |      \n",
      "     |      # Returns\n",
      "     |          A list of numpy arrays.\n",
      "     |  \n",
      "     |  set_weights(self, weights)\n",
      "     |      Sets the weights of the optimizer, from Numpy arrays.\n",
      "     |      \n",
      "     |      Should only be called after computing the gradients\n",
      "     |      (otherwise the optimizer has no weights).\n",
      "     |      \n",
      "     |      # Arguments\n",
      "     |          weights: a list of Numpy arrays. The number\n",
      "     |              of arrays and their shape must match\n",
      "     |              number of the dimensions of the weights\n",
      "     |              of the optimizer (i.e. it should match the\n",
      "     |              output of `get_weights`).\n",
      "     |      \n",
      "     |      # Raises\n",
      "     |          ValueError: in case of incompatible weight shapes.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from Optimizer:\n",
      "     |  \n",
      "     |  from_config(config) from builtins.type\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from Optimizer:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    rmsprop = class RMSprop(Optimizer)\n",
      "     |  RMSProp optimizer.\n",
      "     |  \n",
      "     |  It is recommended to leave the parameters of this optimizer\n",
      "     |  at their default values\n",
      "     |  (except the learning rate, which can be freely tuned).\n",
      "     |  \n",
      "     |  This optimizer is usually a good choice for recurrent\n",
      "     |  neural networks.\n",
      "     |  \n",
      "     |  # Arguments\n",
      "     |      lr: float >= 0. Learning rate.\n",
      "     |      rho: float >= 0.\n",
      "     |      epsilon: float >= 0. Fuzz factor. If `None`, defaults to `K.epsilon()`.\n",
      "     |      decay: float >= 0. Learning rate decay over each update.\n",
      "     |  \n",
      "     |  # References\n",
      "     |      - [rmsprop: Divide the gradient by a running average of its recent magnitude]\n",
      "     |        (http://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf)\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      RMSprop\n",
      "     |      Optimizer\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, lr=0.001, rho=0.9, epsilon=None, decay=0.0, **kwargs)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  get_config(self)\n",
      "     |  \n",
      "     |  get_updates(self, loss, params)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from Optimizer:\n",
      "     |  \n",
      "     |  get_gradients(self, loss, params)\n",
      "     |  \n",
      "     |  get_weights(self)\n",
      "     |      Returns the current value of the weights of the optimizer.\n",
      "     |      \n",
      "     |      # Returns\n",
      "     |          A list of numpy arrays.\n",
      "     |  \n",
      "     |  set_weights(self, weights)\n",
      "     |      Sets the weights of the optimizer, from Numpy arrays.\n",
      "     |      \n",
      "     |      Should only be called after computing the gradients\n",
      "     |      (otherwise the optimizer has no weights).\n",
      "     |      \n",
      "     |      # Arguments\n",
      "     |          weights: a list of Numpy arrays. The number\n",
      "     |              of arrays and their shape must match\n",
      "     |              number of the dimensions of the weights\n",
      "     |              of the optimizer (i.e. it should match the\n",
      "     |              output of `get_weights`).\n",
      "     |      \n",
      "     |      # Raises\n",
      "     |          ValueError: in case of incompatible weight shapes.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from Optimizer:\n",
      "     |  \n",
      "     |  from_config(config) from builtins.type\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from Optimizer:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    sgd = class SGD(Optimizer)\n",
      "     |  Stochastic gradient descent optimizer.\n",
      "     |  \n",
      "     |  Includes support for momentum,\n",
      "     |  learning rate decay, and Nesterov momentum.\n",
      "     |  \n",
      "     |  # Arguments\n",
      "     |      lr: float >= 0. Learning rate.\n",
      "     |      momentum: float >= 0. Parameter that accelerates SGD\n",
      "     |          in the relevant direction and dampens oscillations.\n",
      "     |      decay: float >= 0. Learning rate decay over each update.\n",
      "     |      nesterov: boolean. Whether to apply Nesterov momentum.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      SGD\n",
      "     |      Optimizer\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, lr=0.01, momentum=0.0, decay=0.0, nesterov=False, **kwargs)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  get_config(self)\n",
      "     |  \n",
      "     |  get_updates(self, loss, params)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from Optimizer:\n",
      "     |  \n",
      "     |  get_gradients(self, loss, params)\n",
      "     |  \n",
      "     |  get_weights(self)\n",
      "     |      Returns the current value of the weights of the optimizer.\n",
      "     |      \n",
      "     |      # Returns\n",
      "     |          A list of numpy arrays.\n",
      "     |  \n",
      "     |  set_weights(self, weights)\n",
      "     |      Sets the weights of the optimizer, from Numpy arrays.\n",
      "     |      \n",
      "     |      Should only be called after computing the gradients\n",
      "     |      (otherwise the optimizer has no weights).\n",
      "     |      \n",
      "     |      # Arguments\n",
      "     |          weights: a list of Numpy arrays. The number\n",
      "     |              of arrays and their shape must match\n",
      "     |              number of the dimensions of the weights\n",
      "     |              of the optimizer (i.e. it should match the\n",
      "     |              output of `get_weights`).\n",
      "     |      \n",
      "     |      # Raises\n",
      "     |          ValueError: in case of incompatible weight shapes.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from Optimizer:\n",
      "     |  \n",
      "     |  from_config(config) from builtins.type\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from Optimizer:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "\n",
      "FUNCTIONS\n",
      "    clip_norm(g, c, n)\n",
      "        Clip the gradient `g` if the L2 norm `n` exceeds `c`.\n",
      "        \n",
      "        # Arguments\n",
      "            g: Tensor, the gradient tensor\n",
      "            c: float >= 0. Gradients will be clipped\n",
      "                when their L2 norm exceeds this value.\n",
      "            n: Tensor, actual norm of `g`.\n",
      "        \n",
      "        # Returns\n",
      "            Tensor, the gradient clipped if required.\n",
      "    \n",
      "    deserialize(config, custom_objects=None)\n",
      "        Inverse of the `serialize` function.\n",
      "        \n",
      "        # Arguments\n",
      "            config: Optimizer configuration dictionary.\n",
      "            custom_objects: Optional dictionary mapping\n",
      "                names (strings) to custom objects\n",
      "                (classes and functions)\n",
      "                to be considered during deserialization.\n",
      "        \n",
      "        # Returns\n",
      "            A Keras Optimizer instance.\n",
      "    \n",
      "    get(identifier)\n",
      "        Retrieves a Keras Optimizer instance.\n",
      "        \n",
      "        # Arguments\n",
      "            identifier: Optimizer identifier, one of\n",
      "                - String: name of an optimizer\n",
      "                - Dictionary: configuration dictionary.\n",
      "                - Keras Optimizer instance (it will be returned unchanged).\n",
      "                - TensorFlow Optimizer instance\n",
      "                    (it will be wrapped as a Keras Optimizer).\n",
      "        \n",
      "        # Returns\n",
      "            A Keras Optimizer instance.\n",
      "        \n",
      "        # Raises\n",
      "            ValueError: If `identifier` cannot be interpreted.\n",
      "    \n",
      "    serialize(optimizer)\n",
      "\n",
      "DATA\n",
      "    absolute_import = _Feature((2, 5, 0, 'alpha', 1), (3, 0, 0, 'alpha', 0...\n",
      "    division = _Feature((2, 2, 0, 'alpha', 2), (3, 0, 0, 'alpha', 0), 8192...\n",
      "    print_function = _Feature((2, 6, 0, 'alpha', 2), (3, 0, 0, 'alpha', 0)...\n",
      "\n",
      "FILE\n",
      "    c:\\programdata\\anaconda3\\envs\\neuralnets\\lib\\site-packages\\keras\\optimizers.py\n",
      "\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(help('keras.optimizers'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datagen=ImageDataGenerator(rescale=1. /255,zoom_range=0.1,shear_range=0.1,horizontal_flip=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_datagen=ImageDataGenerator(rescale=1. /255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 672 images belonging to 5 classes.\n"
     ]
    }
   ],
   "source": [
    "train_datagenerator=train_datagen.flow_from_directory(train_dir,batch_size=5,target_size=(input_width,input_height),class_mode='categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 130 images belonging to 5 classes.\n"
     ]
    }
   ],
   "source": [
    "test_datagenerator=test_datagen.flow_from_directory(test_dir,batch_size=5,target_size=(input_width,input_height),class_mode='categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "20/20 [==============================] - 6s 324ms/step - loss: 1.5604 - acc: 0.3000 - val_loss: 1.0873 - val_acc: 0.7000\n",
      "Epoch 2/5\n",
      "20/20 [==============================] - 6s 287ms/step - loss: 0.8557 - acc: 0.7000 - val_loss: 0.7822 - val_acc: 0.8200\n",
      "Epoch 3/5\n",
      "20/20 [==============================] - 6s 285ms/step - loss: 0.6142 - acc: 0.8400 - val_loss: 0.2276 - val_acc: 0.9600\n",
      "Epoch 4/5\n",
      "20/20 [==============================] - 6s 283ms/step - loss: 0.2757 - acc: 0.8800 - val_loss: 0.0607 - val_acc: 1.0000\n",
      "Epoch 5/5\n",
      "20/20 [==============================] - 6s 284ms/step - loss: 0.1173 - acc: 0.9700 - val_loss: 0.1348 - val_acc: 0.9200\n"
     ]
    }
   ],
   "source": [
    "history=model.fit_generator(train_datagenerator,steps_per_epoch=20,epochs=5,validation_data=test_datagenerator,validation_steps=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x2c8f6ac8>"
      ]
     },
     "execution_count": 348,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "acc=history.history['acc']\n",
    "val_acc=history.history['val_acc']\n",
    "epochs=range(len(acc))\n",
    "plt.plot(epochs,acc,label='Training Accuracy')\n",
    "plt.plot(epochs,val_acc,label='Validation Accuracy')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x2c967fd0>"
      ]
     },
     "execution_count": 349,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "loss=history.history['loss']\n",
    "val_loss=history.history['val_loss']\n",
    "plt.plot(epochs,loss,label='Training Loss')\n",
    "plt.plot(epochs,val_loss,label='Validation Loss')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [],
   "source": [
    "probablities=model.predict_generator(test_datagenerator,steps=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "y_pred=model.predict_generator(test_datagenerator,26)\n",
    "y_pred=np.argmax(y_pred,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.16      0.17      0.16        24\n",
      "           1       0.12      0.13      0.13        30\n",
      "           2       0.30      0.23      0.26        30\n",
      "           3       0.06      0.06      0.06        16\n",
      "           4       0.15      0.17      0.16        30\n",
      "\n",
      "   micro avg       0.16      0.16      0.16       130\n",
      "   macro avg       0.16      0.15      0.16       130\n",
      "weighted avg       0.17      0.16      0.16       130\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(test_datagenerator.classes,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 4  7  2  6  5]\n",
      " [ 9  4  3  3 11]\n",
      " [ 4  8  7  4  7]\n",
      " [ 1  5  4  1  5]\n",
      " [ 7  9  7  2  5]]\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(test_datagenerator.classes,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import image\n",
    "from os import listdir\n",
    "from os.path import isfile,join\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [],
   "source": [
    "mypath=\"fruits/images/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [],
   "source": [
    "onlyfiles=[f for f in listdir(mypath) if isfile(join(mypath,f))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "102_100.jpg  : Banana\n",
      "103_100.jpg  : Apple\n",
      "104_100.jpg  : Apple\n",
      "109_100.jpg  : Banana\n",
      "114_100.jpg  : Banana\n",
      "115_100.jpg  : Apple\n",
      "116_100.jpg  : Apple\n",
      "126_100.jpg  : Apple\n",
      "127_100.jpg  : Apple\n",
      "12_100.jpg  : Mango\n",
      "137_100.jpg  : Apple\n",
      "138_100.jpg  : Apple\n",
      "139_100.jpg  : Mango\n",
      "148_100.jpg  : Apple\n",
      "149_100.jpg  : Apple\n",
      "150_100.jpg  : Mango\n",
      "151_100.jpg  : Mango\n",
      "153_100.jpg  : Mango\n",
      "165_100.jpg  : Banana\n",
      "16_100.jpg  : Cactus Fruit\n",
      "196_100.jpg  : Banana\n",
      "209_100.jpg  : Banana\n",
      "249_100.jpg  : Banana\n",
      "258_100.jpg  : Cactus Fruit\n",
      "260_100.jpg  : Mango\n",
      "261_100.jpg  : Banana\n",
      "274_100.jpg  : Mango\n",
      "278_100.jpg  : Mango\n",
      "280_100.jpg  : Banana\n",
      "287_100.jpg  : Cactus Fruit\n",
      "288_100.jpg  : Cactus Fruit\n",
      "298_100.jpg  : Banana\n",
      "299_100.jpg  : Mango\n",
      "300_100.jpg  : Banana\n",
      "303_100.jpg  : Banana\n",
      "304_100.jpg  : Banana\n",
      "308_100.jpg  : Banana\n",
      "311_100.jpg  : Banana\n",
      "312_100.jpg  : Mango\n",
      "314_100.jpg  : Banana\n",
      "315_100.jpg  : Cactus Fruit\n",
      "318_100.jpg  : Cactus Fruit\n",
      "319_100.jpg  : Mango\n",
      "37_100.jpg  : Kiwi\n",
      "38_100.jpg  : Kiwi\n",
      "39_100.jpg  : Kiwi\n",
      "40_100.jpg  : Kiwi\n",
      "48_100.jpg  : Cactus Fruit\n",
      "49_100.jpg  : Kiwi\n",
      "4_100.jpg  : Kiwi\n",
      "51_100.jpg  : Kiwi\n",
      "59_100.jpg  : Kiwi\n",
      "5_100.jpg  : Kiwi\n",
      "60_100.jpg  : Kiwi\n",
      "61_100.jpg  : Kiwi\n",
      "62_100.jpg  : Kiwi\n",
      "6_100.jpg  : Kiwi\n",
      "80_100.jpg  : Apple\n",
      "81_100.jpg  : Apple\n",
      "87_100.jpg  : Banana\n",
      "92_100.jpg  : Apple\n",
      "93_100.jpg  : Apple\n",
      "Kiwi1.jpg  : Kiwi\n",
      "kiwi2.jpg  : Kiwi\n",
      "Mango1.jpg  : Mango\n",
      "r_109_100.jpg  : Cactus Fruit\n",
      "r_115_100.jpg  : Cactus Fruit\n",
      "r_11_100.jpg  : Cactus Fruit\n",
      "r_138_100.jpg  : Cactus Fruit\n",
      "r_14_100.jpg  : Mango\n",
      "r_15_100.jpg  : Cactus Fruit\n",
      "r_16_100.jpg  : Apple\n",
      "r_17_100.jpg  : Apple\n",
      "r_1_100.jpg  : Cactus Fruit\n",
      "r_212_100.jpg  : Mango\n",
      "r_223_100.jpg  : Banana\n",
      "r_226_100.jpg  : Banana\n",
      "r_227_100.jpg  : Banana\n",
      "r_228_100.jpg  : Mango\n",
      "r_230_100.jpg  : Mango\n",
      "r_233_100.jpg  : Mango\n",
      "r_234_100.jpg  : Mango\n",
      "r_236_100.jpg  : Banana\n",
      "r_237_100.jpg  : Mango\n",
      "r_238_100.jpg  : Banana\n",
      "r_243_100.jpg  : Mango\n",
      "r_246_100.jpg  : Mango\n",
      "r_248_100.jpg  : Mango\n",
      "r_252_100.jpg  : Mango\n",
      "r_254_100.jpg  : Mango\n",
      "r_271_100.jpg  : Cactus Fruit\n",
      "r_278_100.jpg  : Cactus Fruit\n",
      "r_27_100.jpg  : Apple\n",
      "r_282_100.jpg  : Cactus Fruit\n",
      "r_284_100.jpg  : Cactus Fruit\n",
      "r_288_100.jpg  : Cactus Fruit\n",
      "r_294_100.jpg  : Cactus Fruit\n",
      "r_295_100.jpg  : Cactus Fruit\n",
      "r_298_100.jpg  : Cactus Fruit\n",
      "r_306_100.jpg  : Cactus Fruit\n",
      "r_308_100.jpg  : Cactus Fruit\n",
      "r_317_100.jpg  : Banana\n",
      "r_318_100.jpg  : Cactus Fruit\n",
      "r_31_100.jpg  : Banana\n",
      "r_320_100.jpg  : Banana\n",
      "r_327_100.jpg  : Banana\n",
      "r_36_100.jpg  : Banana\n",
      "r_39_100.jpg  : Banana\n",
      "r_40_100.jpg  : Banana\n",
      "r_41_100.jpg  : Banana\n",
      "r_43_100.jpg  : Banana\n",
      "r_50_100.jpg  : Banana\n",
      "r_51_100.jpg  : Banana\n",
      "r_67_100.jpg  : Banana\n",
      "r_68_100.jpg  : Banana\n",
      "r_6_100.jpg  : Cactus Fruit\n",
      "r_82_100.jpg  : Mango\n",
      "r_88_100.jpg  : Mango\n"
     ]
    }
   ],
   "source": [
    "for file in listdir(mypath):\n",
    "    img=image.load_img(mypath+file,target_size=(input_width,input_height))\n",
    "    x=image.img_to_array(img)\n",
    "    x=np.expand_dims(x,axis=0)\n",
    "    images=np.vstack([x])\n",
    "    classes=model.predict_classes(images,batch_size=10)\n",
    "    if classes[0]==0:\n",
    "        print(file,\" : Apple\")\n",
    "    elif classes[0]==1:\n",
    "        print(file,\" : Banana\")\n",
    "    elif classes[0]==2:\n",
    "        print(file,\" : Cactus Fruit\")\n",
    "    elif classes[0]==3:\n",
    "        print(file,\" : Kiwi\")\n",
    "    else:\n",
    "        print(file,\" : Mango\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
